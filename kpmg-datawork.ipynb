{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-26T19:26:11.483231Z","iopub.execute_input":"2023-05-26T19:26:11.483760Z","iopub.status.idle":"2023-05-26T19:26:11.501030Z","shell.execute_reply.started":"2023-05-26T19:26:11.483726Z","shell.execute_reply":"2023-05-26T19:26:11.499745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nimport pandas as pd\n\n\ndef dropD(df):\n    return df.drop_duplicates()\n    \ndef getAllInfo(df):\n    \n    #print(df.shape)\n    \n    #print(df.info())\n    \n    #print(df.isna().sum())\n    \n    df_tmp=pd.DataFrame( df.isna().sum() )\n    \n    return (  df_tmp[  (df_tmp.iloc[:,0]>0) ].index )\n\ndef replaceNa(df,k):\n    \n    if type(df[k]) not in ( 'int64' ,'float64' ):\n        df[k].fillna(\"NO VALUE\",inplace=True)\n    else:\n        df[k].fillna(0,inplace=True)\nif __name__==\"__main__\":\n    \n    df0= pd.read_excel(\"C:\\\\KPMG.xlsx\" ,sheet_name='Transactions',header=1)\n    \n    \n    \n    df1=dropD(df0)\n    print(str(df1.shape)+\"  \"+str(df0.shape) )\n    \n    \n    df2=pd.Series( getAllInfo(df1) )\n    print(type(df2))\n    print(df2)\n    \n    for k in df2:\n        replaceNa(df1,k)\n    \n    print(getAllInfo(df1))\n    print(df1.info())\n    \n    \n    xx=df1[\"transaction_date\"].dt.strftime('%d-%m-%Y')\n    \n    from  datetime import date\n    \n    print(  (pd.to_datetime(date.today()) - pd.to_datetime(xx) ).dt.days  )\n    \n    print(  (pd.to_datetime(date.today()) )   )\n    \n    print( df1[\"product_size\"].unique())\n    \n    df1[\"product_size_int\"]=[ 1 if k==\"medium\" else 2 if k==\"large\" else 3 if k==\"small\" else 0 for k in df1[\"product_size\"] ]\n    \n    a=[\"product_size\",\"product_size_int\"]\n    print(df1[a] )\n    ","metadata":{"execution":{"iopub.status.busy":"2023-05-26T19:46:47.156655Z","iopub.execute_input":"2023-05-26T19:46:47.157033Z","iopub.status.idle":"2023-05-26T19:46:47.169407Z","shell.execute_reply.started":"2023-05-26T19:46:47.157004Z","shell.execute_reply":"2023-05-26T19:46:47.168240Z"},"trusted":true},"execution_count":null,"outputs":[]}]}